{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from email.utils import parseaddr\n",
    "from bs4 import BeautifulSoup\n",
    "from spellchecker import SpellChecker\n",
    "from langdetect import detect\n",
    "\n",
    "# Config\n",
    "THREAT_WORDS = [\"urgent\", \"immediately\", \"act now\", \"final notice\", \"account suspended\", \"police\", \"legal action\"]\n",
    "SENSITIVE_KEYWORDS = [\"otp\", \"password\", \"ic number\", \"bank account\", \"credit card\", \"login\", \"verify\"]\n",
    "SUSPICIOUS_DOMAINS = [\".ru\", \".tk\", \".ml\", \".ga\", \"bit.ly\", \"tinyurl\", \"goo.gl\", \"ow.ly\"]\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# === Main Analyzer ===\n",
    "def analyze_email_basic(subject, body, from_email):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    text_body = soup.get_text().lower()\n",
    "\n",
    "    _, sender_email = parseaddr(from_email)\n",
    "    sender_domain = sender_email.split('@')[-1] if '@' in sender_email else ''\n",
    "\n",
    "    flags = {}\n",
    "    metadata = {}\n",
    "\n",
    "    # === Language & Grammar ===\n",
    "    try:\n",
    "        metadata['language'] = detect(text_body)\n",
    "    except:\n",
    "        metadata['language'] = 'unknown'\n",
    "\n",
    "    words = re.findall(r'\\b\\w+\\b', text_body)\n",
    "    misspelled = spell.unknown(words)\n",
    "    flags['grammar_errors'] = len(misspelled) > 5\n",
    "\n",
    "    # === Threatening & Sensitive Language ===\n",
    "    flags['has_threatening_language'] = any(word in text_body for word in THREAT_WORDS)\n",
    "    flags['asks_sensitive_info'] = any(word in text_body for word in SENSITIVE_KEYWORDS)\n",
    "\n",
    "    # === Link Analysis ===\n",
    "    links = [a.get('href') for a in soup.find_all('a', href=True)]\n",
    "    flags['suspicious_links'] = any(any(domain in link for domain in SUSPICIOUS_DOMAINS) for link in links)\n",
    "    flags['url_mismatch'] = any(a.get_text().strip().lower() != a.get('href').strip().lower() for a in soup.find_all('a', href=True))\n",
    "    metadata['num_links'] = len(links)\n",
    "\n",
    "    # === SPF/DKIM Check using Free API ===\n",
    "    # You can use MailCheck API (no API key needed, simple GET)\n",
    "    try:\n",
    "        mailcheck_url = f\"https://mailcheck.co/api/email/{sender_email}\"\n",
    "        response = requests.get(mailcheck_url, timeout=5)\n",
    "        if response.ok:\n",
    "            result = response.json()\n",
    "            flags['spf_pass'] = result.get(\"spf\", {}).get(\"status\") == \"pass\"\n",
    "            flags['dkim_pass'] = result.get(\"dkim\", {}).get(\"status\") == \"pass\"\n",
    "        else:\n",
    "            flags['spf_pass'] = False\n",
    "            flags['dkim_pass'] = False\n",
    "    except:\n",
    "        flags['spf_pass'] = False\n",
    "        flags['dkim_pass'] = False\n",
    "\n",
    "    flags['auth_fail'] = not flags['spf_pass'] or not flags['dkim_pass']\n",
    "\n",
    "    # === Risk Summary ===\n",
    "    score = sum(flag is True for flag in flags.values())\n",
    "    risk = \"High\" if score >= 5 else \"Medium\" if score >= 3 else \"Low\"\n",
    "\n",
    "    return {\n",
    "        \"risk_level\": risk,\n",
    "        \"flags\": flags,\n",
    "        \"metadata\": {\n",
    "            \"from_email\": from_email,\n",
    "            \"sender_domain\": sender_domain,\n",
    "            \"subject\": subject,\n",
    "            \"language\": metadata['language'],\n",
    "            \"num_links\": metadata['num_links']\n",
    "        }\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    subject = \"⚠️ URGENT: Your Bank Account is Suspended!\"\n",
    "    body = \"\"\"\n",
    "    <p>Dear user,</p>\n",
    "    <p>Your account has been suspended. Click <a href='http://verify-safe.tk'>here</a> to verify your IC number now.</p>\n",
    "    <p>Failure to do so will result in legal action.</p>\n",
    "    \"\"\"\n",
    "    from_email = \"support@securebank-alert.com\"\n",
    "\n",
    "    result = analyze_email_basic(subject, body, from_email)\n",
    "    from pprint import pprint\n",
    "    pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    subject = \"⚠️ URGENT: Your Bank Account is Suspended!\"\n",
    "    body = \"\"\"\n",
    "    <p>Dear user,</p>\n",
    "    <p>Your account has been suspended. Click <a href='http://verify-safe.tk'>here</a> to verify your IC number now.</p>\n",
    "    <p>Failure to do so will result in legal action.</p>\n",
    "    \"\"\"\n",
    "    from_email = \"support@securebank-alert.com\"\n",
    "\n",
    "    result = analyze_email_basic(subject, body, from_email)\n",
    "    from pprint import pprint\n",
    "    pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import re\n",
    "import logging\n",
    "from email.header import decode_header, make_header\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import tldextract\n",
    "import language_tool_python\n",
    "import ipaddress # For checking if a hostname is an IP\n",
    "from pprint import pprint # For formatted output\n",
    "\n",
    "# --- Configuration ---\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize LanguageTool (can take a moment on first run)\n",
    "try:\n",
    "    # Ensure the language model is downloaded if needed. Adjust 'en-US' if targeting other languages.\n",
    "    lang_tool = language_tool_python.LanguageTool('en-US')\n",
    "except Exception as e:\n",
    "    logging.warning(f\"Could not initialize LanguageTool. Grammar check disabled. Error: {e}\")\n",
    "    lang_tool = None\n",
    "\n",
    "# Keywords for threat detection (case-insensitive)\n",
    "THREAT_KEYWORDS = [\n",
    "    r'urgent', r'immediate action required', r'account suspension', r'account suspended',\n",
    "    r'security alert', r'unusual activity detected', r'verify your account', r'confirm your identity',\n",
    "    r'locked', r'disabled', r'compromised', r'consequences', r'failure to comply', r'final warning',\n",
    "    r'legal action' # Added based on example\n",
    "]\n",
    "THREAT_PATTERN = re.compile(r'\\b(?:' + '|'.join(THREAT_KEYWORDS) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Keywords for sensitive info requests (case-insensitive)\n",
    "SENSITIVE_INFO_KEYWORDS = [\n",
    "    r'password', r'username', r'login credentials', r'security question', r'mother\\'s maiden name',\n",
    "    r'social security number', r'ssn', r'bank account number', r'routing number', r'credit card number',\n",
    "    r'cvv', r'pin number', r'date of birth', r'dob', r'verify your details', r'update your payment',\n",
    "    r'confirm your information',\n",
    "    r'ic number', r'nric', r'identity card' # Added based on example and common terms\n",
    "]\n",
    "SENSITIVE_INFO_PATTERN = re.compile(r'\\b(?:' + '|'.join(SENSITIVE_INFO_KEYWORDS) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Common URL Shorteners (add more if needed)\n",
    "URL_SHORTENERS = ['bit.ly', 't.co', 'goo.gl', 'tinyurl.com', 'ow.ly', 'buff.ly', 'is.gd', ' cutt.ly']\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def safe_decode_header(header_value):\n",
    "    \"\"\"Safely decodes email headers.\"\"\"\n",
    "    if header_value is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        decoded = decode_header(header_value)\n",
    "        return str(make_header(decoded))\n",
    "    except Exception:\n",
    "        if isinstance(header_value, bytes):\n",
    "            try:\n",
    "                return header_value.decode('utf-8', errors='replace')\n",
    "            except Exception:\n",
    "                 return str(header_value)\n",
    "        return str(header_value)\n",
    "\n",
    "def get_domain_from_email(email_address):\n",
    "    \"\"\"Extracts the registered domain from an email address.\"\"\"\n",
    "    if not email_address or '@' not in email_address:\n",
    "        return None\n",
    "    if '<' in email_address and '>' in email_address:\n",
    "        match = re.search(r'<([^>]+)>', email_address)\n",
    "        if match:\n",
    "            email_address = match.group(1)\n",
    "        else:\n",
    "             email_address = email_address.split('<')[-1].split('>')[0].strip()\n",
    "\n",
    "    try:\n",
    "        domain_part = email_address.split('@')[-1]\n",
    "        ext = tldextract.extract(domain_part)\n",
    "        return ext.registered_domain if ext.registered_domain else domain_part\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Could not extract domain from email '{email_address}': {e}\")\n",
    "        return None\n",
    "\n",
    "def get_domain_from_url(url):\n",
    "    \"\"\"Extracts the registered domain from a URL.\"\"\"\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        hostname = parsed_url.netloc\n",
    "        if not hostname:\n",
    "            return None\n",
    "        hostname = hostname.split(':')[0]\n",
    "        ext = tldextract.extract(hostname)\n",
    "        return ext.registered_domain if ext.registered_domain else hostname\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Could not extract domain from URL '{url}': {e}\")\n",
    "        return None\n",
    "\n",
    "def is_ip_address(hostname):\n",
    "    \"\"\"Checks if a hostname is an IP address using the ipaddress module.\"\"\"\n",
    "    if not hostname: # Handle empty hostnames\n",
    "        return False\n",
    "    try:\n",
    "        ipaddress.ip_address(hostname)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        # Raised if the string is not a valid IP address\n",
    "        return False\n",
    "\n",
    "# --- Main Analysis Function ---\n",
    "\n",
    "def analyze_email_message(raw_email_content):\n",
    "    \"\"\"\n",
    "    Analyzes raw email content (string or bytes) for scam-related metadata.\n",
    "\n",
    "    Args:\n",
    "        raw_email_content: The full raw email content as a string or bytes.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the detected metadata flags.\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"sender_domain_mismatch\": False,\n",
    "        \"suspicious_links\": False,\n",
    "        \"url_mismatch\": False,\n",
    "        \"grammar_errors\": False,\n",
    "        \"has_threatening_language\": False,\n",
    "        \"asks_sensitive_info\": False,\n",
    "        \"spf_fail\": False,\n",
    "        \"dkim_fail\": False\n",
    "    }\n",
    "\n",
    "    if isinstance(raw_email_content, bytes):\n",
    "        try:\n",
    "            raw_email_content = raw_email_content.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                raw_email_content = raw_email_content.decode('latin-1')\n",
    "            except UnicodeDecodeError as e:\n",
    "                logging.error(f\"Could not decode email content: {e}\")\n",
    "                return metadata\n",
    "\n",
    "    try:\n",
    "        msg = email.message_from_string(raw_email_content)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not parse email message: {e}\")\n",
    "        return metadata\n",
    "\n",
    "    # --- 1. Sender Domain Mismatch ---\n",
    "    from_header = safe_decode_header(msg.get('From'))\n",
    "    return_path_header = safe_decode_header(msg.get('Return-Path'))\n",
    "    sender_header = safe_decode_header(msg.get('Sender'))\n",
    "\n",
    "    from_domain = get_domain_from_email(from_header)\n",
    "    envelope_email = return_path_header if return_path_header else sender_header\n",
    "    envelope_domain = get_domain_from_email(envelope_email)\n",
    "\n",
    "    # If Return-Path or Sender exists and differs from From domain\n",
    "    if from_domain and envelope_domain and from_domain != envelope_domain:\n",
    "        logging.info(f\"Sender domain mismatch detected: From='{from_domain}', Envelope='{envelope_domain}'\")\n",
    "        metadata[\"sender_domain_mismatch\"] = True\n",
    "    # If only Sender exists and differs from From domain\n",
    "    elif from_domain and not return_path_header and sender_header:\n",
    "        sender_domain_only = get_domain_from_email(sender_header)\n",
    "        if sender_domain_only and from_domain != sender_domain_only:\n",
    "             logging.info(f\"Sender domain mismatch detected: From='{from_domain}', Sender='{sender_domain_only}'\")\n",
    "             metadata[\"sender_domain_mismatch\"] = True\n",
    "\n",
    "\n",
    "    # --- Initialize body content variables ---\n",
    "    plain_text_body = \"\"\n",
    "    html_body = \"\"\n",
    "    subject = safe_decode_header(msg.get('Subject', '')) # Get subject for keyword checks too\n",
    "\n",
    "\n",
    "    # --- Extract Body Content (Plain Text and HTML) ---\n",
    "    if msg.is_multipart():\n",
    "        for part in msg.walk():\n",
    "            content_type = part.get_content_type()\n",
    "            content_disposition = str(part.get('Content-Disposition'))\n",
    "\n",
    "            if 'attachment' in content_disposition:\n",
    "                continue\n",
    "\n",
    "            if content_type == 'text/plain' and not html_body:\n",
    "                try:\n",
    "                    payload = part.get_payload(decode=True)\n",
    "                    charset = part.get_content_charset() or 'utf-8'\n",
    "                    plain_text_body = payload.decode(charset, errors='replace')\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Could not decode text/plain part: {e}\")\n",
    "\n",
    "            elif content_type == 'text/html':\n",
    "                try:\n",
    "                    payload = part.get_payload(decode=True)\n",
    "                    charset = part.get_content_charset() or 'utf-8'\n",
    "                    html_body = payload.decode(charset, errors='replace')\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Could not decode text/html part: {e}\")\n",
    "    else:\n",
    "        content_type = msg.get_content_type()\n",
    "        try:\n",
    "            payload = msg.get_payload(decode=True)\n",
    "            charset = msg.get_content_charset() or 'utf-8'\n",
    "            body_content = payload.decode(charset, errors='replace')\n",
    "            if content_type == 'text/html':\n",
    "                html_body = body_content\n",
    "            else:\n",
    "                plain_text_body = body_content\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not decode non-multipart body: {e}\")\n",
    "\n",
    "    # Combine text for keyword analysis\n",
    "    html_text_content = \"\"\n",
    "    if html_body:\n",
    "        try:\n",
    "            soup_text = BeautifulSoup(html_body, \"lxml\")\n",
    "            html_text_content = soup_text.get_text(separator=\"\\n\", strip=True)\n",
    "        except Exception as e:\n",
    "             logging.warning(f\"Could not extract text from HTML body: {e}\")\n",
    "\n",
    "    # Include subject in the text checked for keywords\n",
    "    full_text_content = subject + \"\\n\" + plain_text_body + \"\\n\" + html_text_content\n",
    "\n",
    "\n",
    "    # --- 2. Suspicious Links & 3. URL Mismatch ---\n",
    "    if html_body:\n",
    "        try: # Add try-except around BeautifulSoup usage\n",
    "            soup_links = BeautifulSoup(html_body, 'lxml')\n",
    "            links = soup_links.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                href = link.get('href', '').strip() # Use .get() with default\n",
    "                link_text = link.get_text().strip()\n",
    "\n",
    "                if not href or href.startswith('#') or href.startswith('mailto:'):\n",
    "                    continue\n",
    "\n",
    "                parsed_url = urlparse(href)\n",
    "                hostname = parsed_url.netloc.split(':')[0] if parsed_url.netloc else '' # Handle missing netloc\n",
    "                href_domain = get_domain_from_url(href)\n",
    "\n",
    "                # Check for IP address links\n",
    "                if hostname and is_ip_address(hostname):\n",
    "                    logging.info(f\"Suspicious link found (IP Address): {href}\")\n",
    "                    metadata[\"suspicious_links\"] = True\n",
    "\n",
    "                # Check for common URL shorteners\n",
    "                if href_domain and any(shortener in href_domain for shortener in URL_SHORTENERS):\n",
    "                    logging.info(f\"Suspicious link found (URL Shortener): {href}\")\n",
    "                    metadata[\"suspicious_links\"] = True\n",
    "\n",
    "                # Check for URL Mismatch\n",
    "                if link_text.startswith(('http://', 'https://', 'www.')):\n",
    "                     link_text_domain = get_domain_from_url(link_text if link_text.startswith('http') else 'http://' + link_text)\n",
    "                     if href_domain and link_text_domain and href_domain != link_text_domain:\n",
    "                         logging.info(f\"URL Mismatch detected: Text='{link_text}' ({link_text_domain}), Href='{href}' ({href_domain})\")\n",
    "                         metadata[\"url_mismatch\"] = True\n",
    "                         metadata[\"suspicious_links\"] = True # Mismatched URLs are inherently suspicious\n",
    "\n",
    "                # ===> Add check for suspicious TLDs (basic example) <===\n",
    "                suspicious_tlds = ['.tk', '.xyz', '.top', '.loan', '.work', '.info', '.biz'] # Example list\n",
    "                if href_domain and any(href.lower().endswith(tld) for tld in suspicious_tlds):\n",
    "                     # Check if domain itself ends with TLD to avoid subdomain matches\n",
    "                     # Example: verify-safe.tk\n",
    "                     domain_parts = tldextract.extract(href)\n",
    "                     if domain_parts.suffix in suspicious_tlds:\n",
    "                        logging.info(f\"Suspicious link found (Suspicious TLD '.{domain_parts.suffix}'): {href}\")\n",
    "                        metadata[\"suspicious_links\"] = True\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not parse links from HTML body: {e}\")\n",
    "\n",
    "\n",
    "    # --- 4. Grammar Errors ---\n",
    "    if lang_tool and full_text_content.strip():\n",
    "        try:\n",
    "            text_to_check = full_text_content[:5000] # Limit length\n",
    "            matches = lang_tool.check(text_to_check)\n",
    "            error_threshold = max(1, len(text_to_check.split()) // 250) # Slightly more sensitive\n",
    "            if len(matches) > error_threshold:\n",
    "                logging.info(f\"Potential grammar errors detected: {len(matches)} issues found.\")\n",
    "                metadata[\"grammar_errors\"] = True\n",
    "            # Optional: Log the actual errors for debugging\n",
    "            # for match in matches[:5]: # Log first 5 errors\n",
    "            #    logging.debug(f\"Grammar issue: {match.ruleId} - {match.message} -> '{text_to_check[match.offset:match.offset+match.errorLength]}'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch potential errors from the grammar tool itself\n",
    "            logging.warning(f\"Grammar check failed: {e}\")\n",
    "\n",
    "\n",
    "    # --- 5. Threatening Language ---\n",
    "    if THREAT_PATTERN.search(full_text_content):\n",
    "        logging.info(\"Threatening language detected.\")\n",
    "        metadata[\"has_threatening_language\"] = True\n",
    "\n",
    "\n",
    "    # --- 6. Asks Sensitive Info ---\n",
    "    if SENSITIVE_INFO_PATTERN.search(full_text_content):\n",
    "        logging.info(\"Request for sensitive information detected.\")\n",
    "        metadata[\"asks_sensitive_info\"] = True\n",
    "\n",
    "    # --- 7. SPF & DKIM Failures ---\n",
    "    auth_results_header = msg.get_all('Authentication-Results')\n",
    "    if auth_results_header:\n",
    "        for header_line in auth_results_header:\n",
    "            header_line = safe_decode_header(header_line)\n",
    "            if 'spf=fail' in header_line or 'spf=softfail' in header_line:\n",
    "                 logging.info(f\"SPF Fail/Softfail detected in header: {header_line[:200]}...\")\n",
    "                 metadata[\"spf_fail\"] = True\n",
    "            if 'dkim=fail' in header_line:\n",
    "                 logging.info(f\"DKIM Fail detected in header: {header_line[:200]}...\")\n",
    "                 metadata[\"dkim_fail\"] = True\n",
    "\n",
    "\n",
    "    return metadata\n",
    "\n",
    "# --- Example Usage with Your Input ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Your provided input\n",
    "    subject = \"⚠️ URGENT: Your Bank Account is Suspended!\"\n",
    "    body = \"\"\"\n",
    "    <p>Dear user,</p>\n",
    "    <p>Your account has been suspended. Click <a href='http://verify-safe.tk'>here</a> to verify your IC number now.</p>\n",
    "    <p>Failure to do so will result in legal action.</p>\n",
    "    \"\"\"\n",
    "    from_email = \"support@securebank-alert.com\" # Used for the From header\n",
    "\n",
    "    # Construct a minimal raw email string\n",
    "    # Note: No Return-Path or Authentication-Results are added here,\n",
    "    # so sender_domain_mismatch, spf_fail, dkim_fail will be False from this input.\n",
    "    raw_email_input = f\"\"\"From: {from_email}\n",
    "Subject: {subject}\n",
    "Content-Type: text/html; charset=\"utf-8\"\n",
    "MIME-Version: 1.0\n",
    "\n",
    "{body}\n",
    "\"\"\"\n",
    "\n",
    "    print(\"--- Analyzing Provided Email Input ---\")\n",
    "    # Optional: Print the raw email constructed\n",
    "    # print(\"--- Raw Email Constructed ---\")\n",
    "    # print(raw_email_input)\n",
    "    # print(\"--- Analysis Result ---\")\n",
    "\n",
    "    result = analyze_email_message(raw_email_input)\n",
    "    pprint(result)\n",
    "\n",
    "    # Clean up LanguageTool instance if it was created\n",
    "    if lang_tool:\n",
    "        lang_tool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class ScamEmailDetector:\n",
    "    def __init__(self):\n",
    "        # List of sensitive information keywords\n",
    "        self.sensitive_info_keywords = [\n",
    "            'password', 'pin', 'social security', 'ssn', 'credit card', \n",
    "            'account number', 'bank account', 'login', 'username', 'verification code',\n",
    "            'cvv', 'identity card', 'id number', 'passport', 'license number',\n",
    "            'ic number', 'identification'\n",
    "        ]\n",
    "        \n",
    "        # List of threatening phrases\n",
    "        self.threatening_phrases = [\n",
    "            'legal action', 'police', 'lawsuit', 'court', 'suspend', 'terminate',\n",
    "            'urgent', 'warning', 'alert', 'immediately', 'failure to', 'consequence',\n",
    "            'criminal', 'penalty', 'fine', 'restricted', 'blocked', 'unauthorized',\n",
    "            'investigation', 'fraud', 'security breach'\n",
    "        ]\n",
    "        \n",
    "        # Legitimate bank and financial domains\n",
    "        self.legitimate_domains = [\n",
    "            'chase.com', 'bankofamerica.com', 'wellsfargo.com', 'citi.com',\n",
    "            'capitalone.com', 'usbank.com', 'pnc.com', 'tdbank.com', \n",
    "            'paypal.com', 'americanexpress.com', 'discover.com'\n",
    "        ]\n",
    "        \n",
    "        # URL shortener domains to be flagged\n",
    "        self.url_shortener_domains = [\n",
    "            'bit.ly', 'tinyurl.com', 'goo.gl', 't.co', 'ow.ly', \n",
    "            'is.gd', 'buff.ly', 'adf.ly', 'j.mp', 'tiny.cc'\n",
    "        ]\n",
    "        \n",
    "        # Suspicious TLDs\n",
    "        self.suspicious_tlds = [\n",
    "            '.tk', '.ml', '.ga', '.cf', '.gq', '.xyz', '.top', '.club',\n",
    "            '.online', '.site', '.icu', '.fun', '.pw', '.buzz'\n",
    "        ]\n",
    "        \n",
    "        # Common grammar errors found in phishing emails\n",
    "        self.grammar_error_patterns = [\n",
    "            r'(?i)kindly\\s+(?:verify|confirm|validate)',\n",
    "            r'(?i)please\\s+(?:urgent|immediate)',\n",
    "            r'(?i)do\\s+needful',\n",
    "            r'(?i)revert\\s+back',\n",
    "            r'(?i)we\\s+(?:detected|noticed)\\s+suspicious',\n",
    "        ]\n",
    "\n",
    "    def check_sender_domain_mismatch(self, from_email):\n",
    "        \"\"\"Check if sender email domain matches common financial institution domains.\"\"\"\n",
    "        try:\n",
    "            domain = from_email.split('@')[-1].lower()\n",
    "            \n",
    "            # Check for suspicious features in sender domain\n",
    "            for legitimate_domain in self.legitimate_domains:\n",
    "                # Check if trying to imitate a legitimate domain with slight modifications\n",
    "                if legitimate_domain in domain and domain != legitimate_domain:\n",
    "                    return True\n",
    "                    \n",
    "            # Check for suspicious keywords in domain\n",
    "            suspicious_keywords = ['secure', 'bank', 'verify', 'alert', 'support', 'confirm']\n",
    "            domain_parts = domain.split('.')\n",
    "            for part in domain_parts:\n",
    "                for keyword in suspicious_keywords:\n",
    "                    if keyword.lower() in part.lower():\n",
    "                        # If contains bank-like words but not in our legitimate list\n",
    "                        if domain not in self.legitimate_domains:\n",
    "                            return True\n",
    "            \n",
    "            return False\n",
    "        except Exception:\n",
    "            # If we can't parse the email, assume it's suspicious\n",
    "            return True\n",
    "\n",
    "    def extract_urls_from_html(self, html_content):\n",
    "        \"\"\"Extract all URLs from HTML content.\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            urls = []\n",
    "            \n",
    "            # Extract URLs from anchor tags\n",
    "            for a_tag in soup.find_all('a'):\n",
    "                href = a_tag.get('href')\n",
    "                if href and not href.startswith('#') and not href.startswith('mailto:'):\n",
    "                    urls.append(href)\n",
    "                    \n",
    "            # Extract URLs from images\n",
    "            for img_tag in soup.find_all('img'):\n",
    "                src = img_tag.get('src')\n",
    "                if src:\n",
    "                    urls.append(src)\n",
    "                    \n",
    "            # Extract URLs that might be in the text using regex\n",
    "            text_content = soup.get_text()\n",
    "            url_pattern = r'https?://[^\\s<>\"\\']+|www\\.[^\\s<>\"\\']+'\n",
    "            urls.extend(re.findall(url_pattern, text_content))\n",
    "            \n",
    "            return urls\n",
    "        except Exception:\n",
    "            # If we can't parse the HTML, return an empty list\n",
    "            return []\n",
    "\n",
    "    def check_suspicious_links(self, html_content):\n",
    "        \"\"\"Check for suspicious links in email body.\"\"\"\n",
    "        urls = self.extract_urls_from_html(html_content)\n",
    "        \n",
    "        if not urls:\n",
    "            return False\n",
    "            \n",
    "        for url in urls:\n",
    "            try:\n",
    "                parsed_url = urlparse(url)\n",
    "                domain = parsed_url.netloc\n",
    "                \n",
    "                # If no domain is found, try to parse again with scheme\n",
    "                if not domain and not url.startswith(('http://', 'https://')):\n",
    "                    parsed_url = urlparse(f\"http://{url}\")\n",
    "                    domain = parsed_url.netloc\n",
    "                \n",
    "                # Check against URL shorteners\n",
    "                if any(shortener in domain for shortener in self.url_shortener_domains):\n",
    "                    return True\n",
    "                    \n",
    "                # Check against suspicious TLDs\n",
    "                if any(domain.endswith(tld) for tld in self.suspicious_tlds):\n",
    "                    return True\n",
    "                    \n",
    "                # Flag IP addresses instead of domains\n",
    "                if re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', domain):\n",
    "                    return True\n",
    "                    \n",
    "                # Check for subdomains mimicking legitimate domains\n",
    "                for legitimate_domain in self.legitimate_domains:\n",
    "                    if legitimate_domain in domain and domain != legitimate_domain:\n",
    "                        return True\n",
    "            except Exception:\n",
    "                # Count malformed URLs as suspicious\n",
    "                return True\n",
    "                \n",
    "        return False\n",
    "\n",
    "    def check_url_mismatch(self, html_content):\n",
    "        \"\"\"Check for URL text vs href mismatches.\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            for a_tag in soup.find_all('a'):\n",
    "                href = a_tag.get('href')\n",
    "                link_text = a_tag.get_text().strip()\n",
    "                \n",
    "                if not href or not link_text:\n",
    "                    continue\n",
    "                    \n",
    "                # Skip if link text is generic like \"here\" or \"click here\"\n",
    "                if link_text.lower() in ['here', 'click here', 'this link']:\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    # Parse the href URL\n",
    "                    parsed_href = urlparse(href)\n",
    "                    \n",
    "                    # Checking if the link text contains a URL\n",
    "                    if re.search(r'https?://[^\\s<>\"\\']+|www\\.[^\\s<>\"\\']', link_text):\n",
    "                        parsed_text = urlparse(link_text if link_text.startswith(('http://', 'https://')) else f\"http://{link_text}\")\n",
    "                        \n",
    "                        # Compare domains\n",
    "                        if parsed_href.netloc and parsed_text.netloc and parsed_href.netloc != parsed_text.netloc:\n",
    "                            return True\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            return False\n",
    "        except Exception:\n",
    "            # If we can't parse the HTML, assume it's safe\n",
    "            return False\n",
    "\n",
    "    def check_grammar_errors(self, text):\n",
    "        \"\"\"Check for common grammar errors and patterns in scam emails.\"\"\"\n",
    "        try:\n",
    "            # Extract text from HTML if needed\n",
    "            if '<' in text and '>' in text:\n",
    "                soup = BeautifulSoup(text, 'html.parser')\n",
    "                text = soup.get_text()\n",
    "                \n",
    "            # Check for common grammar error patterns\n",
    "            for pattern in self.grammar_error_patterns:\n",
    "                if re.search(pattern, text):\n",
    "                    return True\n",
    "            \n",
    "            # Check for excessive punctuation or capitalization\n",
    "            if re.search(r'[!]{3,}', text) or re.search(r'[A-Z]{5,}', text):\n",
    "                return True\n",
    "                \n",
    "            return False\n",
    "        except Exception:\n",
    "            # If we can't check grammar, assume it's safe\n",
    "            return False\n",
    "\n",
    "    def check_threatening_language(self, text):\n",
    "        \"\"\"Check for threatening language in the email.\"\"\"\n",
    "        try:\n",
    "            # Extract text from HTML if needed\n",
    "            if '<' in text and '>' in text:\n",
    "                soup = BeautifulSoup(text, 'html.parser')\n",
    "                text = soup.get_text().lower()\n",
    "                \n",
    "            for phrase in self.threatening_phrases:\n",
    "                if phrase.lower() in text:\n",
    "                    return True\n",
    "                    \n",
    "            return False\n",
    "        except Exception:\n",
    "            # If we can't check the text, assume it's safe\n",
    "            return False\n",
    "\n",
    "    def check_sensitive_info_requests(self, text):\n",
    "        \"\"\"Check if the email asks for sensitive information.\"\"\"\n",
    "        try:\n",
    "            # Extract text from HTML if needed\n",
    "            if '<' in text and '>' in text:\n",
    "                soup = BeautifulSoup(text, 'html.parser')\n",
    "                text = soup.get_text().lower()\n",
    "                \n",
    "            # Check for sensitive info keywords\n",
    "            for keyword in self.sensitive_info_keywords:\n",
    "                if keyword.lower() in text:\n",
    "                    return True\n",
    "                    \n",
    "            # Look for input fields in HTML forms that might request sensitive info\n",
    "            if '<' in text and '>' in text:\n",
    "                soup = BeautifulSoup(text, 'html.parser')\n",
    "                for input_field in soup.find_all('input'):\n",
    "                    field_type = input_field.get('type', '')\n",
    "                    field_name = input_field.get('name', '')\n",
    "                    field_id = input_field.get('id', '')\n",
    "                    field_placeholder = input_field.get('placeholder', '')\n",
    "                    \n",
    "                    # Check all attributes for sensitive keywords\n",
    "                    field_attrs = f\"{field_type} {field_name} {field_id} {field_placeholder}\".lower()\n",
    "                    for keyword in self.sensitive_info_keywords:\n",
    "                        if keyword.lower() in field_attrs:\n",
    "                            return True\n",
    "                            \n",
    "            return False\n",
    "        except Exception:\n",
    "            # If we can't check the text, assume it's safe\n",
    "            return False\n",
    "    \n",
    "    def simulate_spf_dkim_check(self, from_email):\n",
    "        \"\"\"\n",
    "        Simulate SPF and DKIM verification failures.\n",
    "        \n",
    "        In a real implementation, this would check actual email headers.\n",
    "        For simplicity, we'll just simulate authentication failures for\n",
    "        suspicious-looking domains.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            domain = from_email.split('@')[-1].lower()\n",
    "            \n",
    "            # Suspicious patterns that would likely fail authentication\n",
    "            suspicious_patterns = [\n",
    "                '-alert', 'secure', 'verify', 'notification', 'update',\n",
    "                'confirm', 'support', 'service', 'account'\n",
    "            ]\n",
    "            \n",
    "            for pattern in suspicious_patterns:\n",
    "                if pattern in domain and domain not in self.legitimate_domains:\n",
    "                    return True  # Authentication likely would fail\n",
    "                    \n",
    "            # Special case for the example domain\n",
    "            if domain == \"securebank-alert.com\":\n",
    "                return True\n",
    "                \n",
    "            return False  # Assume authentication would pass\n",
    "        except Exception:\n",
    "            # If we can't check, assume authentication fails\n",
    "            return True\n",
    "\n",
    "def analyze_email_basic(subject, body, from_email):\n",
    "    \"\"\"\n",
    "    Analyze an email for potential scam indicators.\n",
    "    \n",
    "    Args:\n",
    "        subject: Email subject line\n",
    "        body: Email body content (can be HTML)\n",
    "        from_email: Sender email address\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with scam detection results in the requested format\n",
    "    \"\"\"\n",
    "    detector = ScamEmailDetector()\n",
    "    \n",
    "    # Extract sender domain\n",
    "    sender_domain = from_email.split('@')[-1] if '@' in from_email else \"\"\n",
    "    \n",
    "    # Count links\n",
    "    num_links = len(detector.extract_urls_from_html(body))\n",
    "    \n",
    "    # Perform all checks\n",
    "    sender_domain_mismatch = detector.check_sender_domain_mismatch(from_email)\n",
    "    suspicious_links = detector.check_suspicious_links(body)\n",
    "    url_mismatch = detector.check_url_mismatch(body)\n",
    "    grammar_errors = detector.check_grammar_errors(body)\n",
    "    threatening_language = detector.check_threatening_language(body)\n",
    "    asks_sensitive_info = detector.check_sensitive_info_requests(body)\n",
    "    auth_fail = detector.simulate_spf_dkim_check(from_email)\n",
    "    \n",
    "    # Create result in the requested format\n",
    "    result = {\n",
    "        'flags': {\n",
    "            'asks_sensitive_info': asks_sensitive_info,\n",
    "            'auth_fail': auth_fail,\n",
    "            'dkim_pass': not auth_fail,  # Simplified - in reality, SPF and DKIM are separate\n",
    "            'grammar_errors': grammar_errors,\n",
    "            'has_threatening_language': threatening_language,\n",
    "            'spf_pass': not auth_fail,   # Simplified\n",
    "            'suspicious_links': suspicious_links,\n",
    "            'url_mismatch': url_mismatch\n",
    "        },\n",
    "        'metadata': {\n",
    "            'from_email': from_email,\n",
    "            'language': 'en',  # Assuming English for simplicity\n",
    "            'num_links': num_links,\n",
    "            'sender_domain': sender_domain,\n",
    "            'subject': subject\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Determine risk level\n",
    "    risk_factors = sum([\n",
    "        asks_sensitive_info,\n",
    "        auth_fail,\n",
    "        grammar_errors,\n",
    "        threatening_language, \n",
    "        suspicious_links,\n",
    "        url_mismatch,\n",
    "        sender_domain_mismatch\n",
    "    ])\n",
    "    \n",
    "    if risk_factors >= 4:\n",
    "        result['risk_level'] = 'High'\n",
    "    elif risk_factors >= 2:\n",
    "        result['risk_level'] = 'Medium'\n",
    "    else:\n",
    "        result['risk_level'] = 'Low'\n",
    "        \n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    subject = \"⚠️ URGENT: Your Bank Account is Suspended!\"\n",
    "    body = \"\"\"\n",
    "    <p>Dear user,</p>\n",
    "    <p>Your account has been suspended. Click <a href='http://verify-safe.tk'>here</a> to verify your IC number now.</p>\n",
    "    <p>Failure to do so will result in legal action.</p>\n",
    "    \"\"\"\n",
    "    from_email = \"support@securebank-alert.com\"\n",
    "\n",
    "    result = analyze_email_basic(subject, body, from_email)\n",
    "    print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flags': {'asks_sensitive_info': True,\n",
      "           'grammar_errors': False,\n",
      "           'has_threatening_language': True,\n",
      "           'suspicious_links': True,\n",
      "           'url_mismatch': True},\n",
      " 'metadata': {'from_email': 'support@securebank-alert.com',\n",
      "              'language': 'en',\n",
      "              'num_links': 1,\n",
      "              'sender_domain': 'securebank-alert.com',\n",
      "              'subject': '⚠️ URGENT: Your Bank Account is Suspended!'},\n",
      " 'risk_level': 'High'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from email.utils import parseaddr\n",
    "from bs4 import BeautifulSoup\n",
    "from spellchecker import SpellChecker\n",
    "from langdetect import detect\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# === Config ===\n",
    "THREAT_KEYWORDS = [\n",
    "    \"urgent\", \"immediate action required\", \"account suspension\", \"account suspended\",\n",
    "    \"security alert\", \"unusual activity\", \"verify your account\", \"legal action\", \"final notice\"\n",
    "]\n",
    "\n",
    "SENSITIVE_KEYWORDS = [\n",
    "    \"otp\", \"password\", \"ic number\", \"bank account\", \"credit card\", \"login\", \"verify\",\n",
    "    \"security question\", \"ssn\", \"cvv\", \"nric\", \"identity card\"\n",
    "]\n",
    "\n",
    "URL_SHORTENERS = ['bit.ly', 't.co', 'goo.gl', 'tinyurl.com', 'ow.ly', 'buff.ly', 'is.gd', 'cutt.ly']\n",
    "SUSPICIOUS_TLDS = ['.tk', '.ml', '.ga', '.ru', '.cn', '.xyz', '.top', '.loan']\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# === Helper Functions ===\n",
    "def get_links_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "    return links, soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "def extract_domain(url):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        return parsed.netloc.lower()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def has_suspicious_tld(domain):\n",
    "    return any(domain.endswith(tld) for tld in SUSPICIOUS_TLDS)\n",
    "\n",
    "def uses_url_shortener(domain):\n",
    "    return domain in URL_SHORTENERS\n",
    "\n",
    "# === Main Analyzer ===\n",
    "def analyze_email(subject, body, from_email):\n",
    "    _, sender_email = parseaddr(from_email)\n",
    "    sender_domain = sender_email.split('@')[-1] if '@' in sender_email else ''\n",
    "    \n",
    "    # --- Extract text and links ---\n",
    "    links, text_body = get_links_from_html(body)\n",
    "    text_lower = (subject + \"\\n\" + text_body).lower()\n",
    "\n",
    "    # --- Flags ---\n",
    "    flags = {}\n",
    "\n",
    "    # Grammar errors\n",
    "    words = re.findall(r'\\b\\w+\\b', text_lower)\n",
    "    misspelled = spell.unknown(words)\n",
    "    flags['grammar_errors'] = len(misspelled) > 5\n",
    "\n",
    "    # Threat & sensitive language\n",
    "    flags['has_threatening_language'] = any(kw in text_lower for kw in THREAT_KEYWORDS)\n",
    "    flags['asks_sensitive_info'] = any(kw in text_lower for kw in SENSITIVE_KEYWORDS)\n",
    "\n",
    "    # Link analysis\n",
    "    flags['suspicious_links'] = False\n",
    "    flags['url_mismatch'] = False\n",
    "    for a in links:\n",
    "        href = a['href'].strip()\n",
    "        link_text = a.get_text().strip()\n",
    "        domain = extract_domain(href)\n",
    "\n",
    "        if has_suspicious_tld(domain) or uses_url_shortener(domain):\n",
    "            flags['suspicious_links'] = True\n",
    "\n",
    "        if link_text.startswith(\"http\") or link_text.startswith(\"www.\"):\n",
    "            text_domain = extract_domain(link_text)\n",
    "            if text_domain and text_domain != domain:\n",
    "                flags['url_mismatch'] = True\n",
    "                flags['suspicious_links'] = True\n",
    "\n",
    "    # --- Metadata ---\n",
    "    try:\n",
    "        lang = detect(text_lower)\n",
    "    except:\n",
    "        lang = 'unknown'\n",
    "\n",
    "    metadata = {\n",
    "        \"from_email\": from_email,\n",
    "        \"sender_domain\": sender_domain,\n",
    "        \"subject\": subject,\n",
    "        \"language\": lang,\n",
    "        \"num_links\": len(links)\n",
    "    }\n",
    "\n",
    "    # --- Risk Summary ---\n",
    "    score = sum(flag is True for flag in flags.values())\n",
    "    risk = \"High\" if score >= 4 else \"Medium\" if score >= 2 else \"Low\"\n",
    "\n",
    "    return {\n",
    "        \"risk_level\": risk,\n",
    "        \"flags\": flags,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "\n",
    "# === Example Usage ===\n",
    "if __name__ == \"__main__\":\n",
    "    subject = \"⚠️ URGENT: Your Bank Account is Suspended!\"\n",
    "    body = \"\"\"\n",
    "    <p>Dear user,</p>\n",
    "    <p>Your account has been suspended. Click <a href='http://verify-safe.tk'>https://www.maybank.com</a> to verify your IC number now.</p>\n",
    "    <p>Failure to do so will result in legal action.</p>\n",
    "    \"\"\"\n",
    "    from_email = \"support@securebank-alert.com\"\n",
    "\n",
    "    result = analyze_email(subject, body, from_email)\n",
    "    from pprint import pprint\n",
    "    pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123777132      → Unknown\n",
      "+60123777132    → Mobile\n",
      "03-77282920     → Unknown\n",
      "1300131300      → Unknown\n",
      "60012345678     → Unknown\n",
      "12345           → Unknown\n",
      "abcdef          → Unknown\n",
      "                → Unknown\n"
     ]
    }
   ],
   "source": [
    "import phonenumbers\n",
    "\n",
    "def classify_phone_number_robust(phone_number):\n",
    "    try:\n",
    "        parsed = phonenumbers.parse(phone_number, \"MY\")  # Adjust region if needed\n",
    "        if phonenumbers.is_possible_number(parsed) and phonenumbers.is_valid_number(parsed):\n",
    "            num_type = phonenumbers.number_type(parsed)\n",
    "            if num_type == phonenumbers.PhoneNumberType.MOBILE:\n",
    "                return \"Mobile\"\n",
    "            elif num_type == phonenumbers.PhoneNumberType.FIXED_LINE:\n",
    "                return \"Landline\"\n",
    "            elif num_type == phonenumbers.PhoneNumberType.TOLL_FREE:\n",
    "                return \"Toll-Free\"\n",
    "            elif num_type == phonenumbers.PhoneNumberType.SHORT_CODE:\n",
    "                return \"Short Code\"\n",
    "            else:\n",
    "                return \"Other\"\n",
    "        else:\n",
    "            return \"Invalid\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# === Test Numbers ===\n",
    "test_numbers = [\n",
    "    \"0123777132\",        # Malaysian mobile (may not work unless intl format)\n",
    "    \"+60123777132\",      # Proper international mobile format\n",
    "    \"03-77282920\",       # Landline\n",
    "    \"1300131300\",        # Toll-Free\n",
    "    \"60012345678\",       # Premium?\n",
    "    \"12345\",             # Short Code?\n",
    "    \"abcdef\",            # Junk\n",
    "    \"\",                  # Empty\n",
    "]\n",
    "\n",
    "for num in test_numbers:\n",
    "    print(f\"{num:15} → {classify_phone_number_robust(num)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/zhangyuxuan/Desktop/cleaned_SMS.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel saved to sms_model.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     X, y = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# Split train/test\u001b[39;00m\n\u001b[32m     49\u001b[39m     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_data\u001b[39m():\n\u001b[32m     12\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m    Reads the CSV file, maps Label=SPAM->0, HAM->1.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m    Returns X (messages) and y (labels).\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/Users/zhangyuxuan/Desktop/cleaned_SMS.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[32m     17\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mLabel\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mLabel\u001b[39m\u001b[33m\"\u001b[39m].map({\u001b[33m\"\u001b[39m\u001b[33mSPAM\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHAM\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m})\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df[\u001b[33m\"\u001b[39m\u001b[33mMessage\u001b[39m\u001b[33m\"\u001b[39m], df[\u001b[33m\"\u001b[39m\u001b[33mLabel\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MonashAI\\Y2S2\\FIT5120\\Iteration 1\\ScamDetek\\ScamDetek\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MonashAI\\Y2S2\\FIT5120\\Iteration 1\\ScamDetek\\ScamDetek\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MonashAI\\Y2S2\\FIT5120\\Iteration 1\\ScamDetek\\ScamDetek\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MonashAI\\Y2S2\\FIT5120\\Iteration 1\\ScamDetek\\ScamDetek\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MonashAI\\Y2S2\\FIT5120\\Iteration 1\\ScamDetek\\ScamDetek\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/zhangyuxuan/Desktop/cleaned_SMS.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Reads the CSV file, maps Label=SPAM->0, HAM->1.\n",
    "    Returns X (messages) and y (labels).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"/Users/zhangyuxuan/Desktop/cleaned_SMS.csv\")  \n",
    "    df[\"Label\"] = df[\"Label\"].map({\"SPAM\": 0, \"HAM\": 1})\n",
    "    return df[\"Message\"], df[\"Label\"]\n",
    "\n",
    "def build_pipeline():\n",
    "    \"\"\"\n",
    "    Builds the Pipeline with TF-IDF + XGBoost.\n",
    "    You can hardcode your best params here.\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            max_df=0.8,\n",
    "            min_df=2,\n",
    "            max_features=3000,\n",
    "            ngram_range=(1,1)\n",
    "        )),\n",
    "        ('xgb', xgb.XGBClassifier(\n",
    "            colsample_bytree=1.0,\n",
    "            learning_rate=0.3,\n",
    "            max_depth=6,\n",
    "            n_estimators=200,\n",
    "            subsample=1.0,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='mlogloss'\n",
    "        ))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    X, y = load_data()\n",
    "\n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build pipeline\n",
    "    model = build_pipeline()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=[\"SPAM\", \"HAM\"])\n",
    "\n",
    "    print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
    "    print(report)\n",
    "\n",
    "    # save model\n",
    "    joblib.dump(model, \"sms_model.pkl\")\n",
    "    print(\"Model saved to sms_model.pkl\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
